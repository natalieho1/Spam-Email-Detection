{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8785686,"sourceType":"datasetVersion","datasetId":5281725}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade tensorflow\n!pip install --upgrade keras ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport string \nimport nltk\nfrom nltk.corpus import stopwords \nnltk.download('stopwords')\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom sklearn.model_selection import train_test_split \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install imbalanced-learn \nfrom imblearn.over_sampling import RandomOverSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham = pd.read_csv('/kaggle/input/sodone/Ham 2.csv')\nspam = pd.read_csv('/kaggle/input/sodone/Spam 2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham['text'] = ham[\"From\"] + ' ' + ham[\"Subject\"] + \" \" + ham[\"Snippet\"]\nham = ham.drop(columns=[\"From\", \"Subject\", \"Snippet\", \"ID\", \"Thread\", \"Date\", \"To\", \"Labels\", \"Link\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spam['text'] = spam[\"From\"] + ' ' + spam[\"Subject\"] + \" \" + spam[\"Snippet\"]\nspam = spam.drop(columns=[\"From\", \"Subject\", \"Snippet\", \"ID\", \"Thread\", \"Date\", \"To\", \"Labels\", \"Link\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ham['label'] = 0 \nspam['label'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined = pd.concat([ham, spam], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separate the data into features and labels ","metadata":{}},{"cell_type":"code","source":"features = combined[\"text\"]\nlabels = combined[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove stopwords \ndef remove_stopwords(text):\n    #print(text)\n    stop_words = stopwords.words('english')\n    star_words = []\n    \n    #store important words\n    for word in str(text).split():\n            word = word.lower()\n            if word not in stop_words:\n                star_words.append(word)\n        #print(star_words)\n    output = \" \".join(star_words)\n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined[\"text\"] = combined[\"text\"].apply(lambda row: remove_stopwords(row))\nprint(features)\ncombined.head(6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove punctuations ","metadata":{}},{"cell_type":"code","source":"punctuations_list = string.punctuation \ndef remove_punctuations(text):\n    for email in text: \n        email = str.maketrans('', '', punctuations_list)\n        #print(text.translate(email))\n    return text.translate(email)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined[\"text\"] = combined[\"text\"].apply(lambda row: remove_punctuations(row))\ncombined","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined[\"label\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Balance the dataset ","metadata":{}},{"cell_type":"code","source":"ros = RandomOverSampler(random_state = 42)\nfeatures_resampled, labels_resampled = ros.fit_resample(combined[[\"text\"]], combined[\"label\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_resampled.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert resampled features back into a DataFrame \nfeatures_resampled = pd.DataFrame(features_resampled, columns=[\"text\"])\nlabels_resampled = pd.DataFrame(labels_resampled, columns=[\"label\"])\nresampled = pd.concat([features_resampled, labels_resampled], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec Conversion ","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(resampled, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = train[\"text\"]\ntrain_Y = train[\"label\"]\ntest_X = test[\"text\"]\ntest_Y = test[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_Y.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_Y.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenize the data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_X)\n\n#Convert text to sequences \ntrain_sequences = tokenizer.texts_to_sequences(train_X)\ntest_sequences = tokenizer.texts_to_sequences(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 100 \ntrain_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\ntest_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_sequences.shape)\ntest_sequences.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len))\nmodel.add(tf.keras.layers.LSTM(128))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = len(tokenizer.word_index) + 1\nprint(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_sequences, train_Y, epochs=10,batch_size=32, validation_split=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_sequences, test_Y)\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}